Assumptions:

Architectural:
1. the PostgreSQL database, the Groq LLM API, and all network connections are always available and performaning. 
   In a production environment, we'd need error handling for API outages, database connection failures, etc.
2. Stateful Memory is Critical, so I assumed that a stateless, turn-by-turn agent would be insufficient. 
   Therefore, I implemented two forms of memory:
   a. Short-Term Memory (active_ticket_id): To handle direct follow-up questions ("what is the solution for that?").
   b. Long-Term Memory (Graph Database): To retain the full conversation history for context.
3. I assumed that classifying the user's intent before taking any other action is the most efficient model for orchestrating distinct tasks 
   (e.g., fetching a ticket vs. searching documentation).

Knowledge base assumptions:
1. I assumed that the mathematical "closeness" of vectors generated by the all-MiniLM-L6-v2 model is a reliable proxy for how semantically related two pieces of text are. 
   The success of every knowledge-based RAG lookup depends on this.
2. the way the chunks are broken down the documentation(kb) into smaller chunks creates pieces, are self-contained and meaningful enough to answer questions.

AI assumptions:
1. I assumed the LLM (Groq's Llama 3) can reliably adhere to the multi-step, rule-based instructions in the rag_system_prompt. 
   This includes complex behaviors like synthesizing answers instead of providing links, formatting lists, 
   and offering to create tickets under specific conditions. This is the foundation of modern prompt engineering.
2. I assumed that a fast LLM can accurately categorize user queries into the predefined intents and can effectively 
   transform conversational language into optimized search queries for the vector database.
   
Usage assumptions
1. Users are interacting with text and not with image/screenshots.
2. Users are logged in when using the tool.
3. There is no more than 30 RPM


Executive Overview:
The objective of this project was to develop a customer support agent to address user issues by leveraging a knowledge base and integrating with a system of record for ticket management. The core requirement was to build an AI agent capable of initiating a chat, retrieving ticket details provided by a customer, identifying relevant information from the knowledge base, and addressing the customer's concerns in a conversational manner.
I have successfully delivered a stateful, context-aware agent that meets and significantly exceeds these original requirements. The final system not only retrieves specific tickets but also intelligently manages conversation history, allowing for natural follow-up questions. Critically, the tool has evolved beyond the initial scope by implementing several advanced features that dramatically enhance the user experience and automation potential. These include:

1. Proactive Ticket Creation: When the knowledge base is insufficient to solve a user's new issue, the agent proactively offers to create a ticket.
2. Full Ticket History Listing: Users can request a complete list of all their support tickets, not just a single one.
3. Conversational Memory: The agent remembers the immediate context of the conversation (e.g., which ticket is being discussed), allowing for intuitive interactions like "what is the solution for that?".
4. True Answer Synthesis: The agent provides direct answers and solutions synthesized from the knowledge base, rather than simply returning links to documentation.

The agent's hybrid architecture, combining Retrieval-Augmented Generation (RAG) and Conversationally-Augmented Generation (CAG), provides a robust foundation for delivering instant, accurate and contextually aware support.


Supporting Documentation on the Solution:
The solution is a multi-component system orchestrated by the main agent logic. The documentation is broken down by component.

Architectural Overview: A Hybrid RAG + CAG Model
To meet the project's requirements for both factual accuracy and conversational fluency, we implemented a hybrid architecture combining two key AI paradigms:

1. Retrieval-Augmented Generation (RAG): The Agent's Factual Memory. This directly addresses the requirement to "identify relevant information from knowledge base." RAG grounds the agent in facts by first retrieving relevant documents from the pg_docs vector database. The LLM is then instructed to synthesize its answer based only on this retrieved information. This prevents the agent from "hallucinating" or providing incorrect information and ensures its answers are always backed by the authoritative documentation.

2. Conversationally-Augmented Generation (CAG): The Agent's Working Memory. This addresses the requirement to "address customer concerns" in a natural, multi-turn chat. CAG provides the agent with the context of the current conversation. This is implemented through the graph-based history (Apache AGE) and the active_ticket_id state. This allows the agent to understand ambiguous follow-up questions like "can you give me a solution for that?", because it remembers what "that" refers to from the previous turn.

By combining RAG and CAG, I have created an agent that is both factually accurate and conversationally intelligent.

Technology Stack Justification:
The technology stack was chosen to create a simplified, powerful, and unified data backend, minimizing architectural complexity and operational overhead.

1. Why PostgreSQL?
PostgreSQL was selected not just as the relational database but as the unified data platform for the entire project.

    a. System of Record: It is a world-class, production-ready database, making it the perfect choice for the tickets table, fulfilling the requirement for a robust "System of Record."

    b. Integrated Vector Database (pgvector): The pgvector extension allows PostgreSQL to store and query vector embeddings efficiently. This allowed me to co-locate the knowledge base vectors (pg_docs) alongside the structured ticket data. This eliminates the need for a separate, dedicated vector database (like Pinecone or Weaviate or chromaDB), dramatically simplifying the architecture, reducing costs, and avoiding data synchronization issues.

    c. Integrated Graph Database (Apache AGE): The Apache AGE extension enables graph database capabilities directly within PostgreSQL. I leveraged this to store the conversation history, providing a powerful and scalable solution for the agent's long-term memory without adding yet another database to the stack.

2. Why the Rest of the Stack?

    a. Python: The de facto language for AI/ML development with an unparalleled ecosystem of libraries.

    b. Groq: Chosen for its LPUâ„¢ Inference Engine, which provides extremely low latency. This is critical for a real-time chat application to ensure the user experience is fluid and responsive. This two-model strategy (a small, fast model for classification and a large, powerful model for synthesis) optimizes for both speed and quality. Furthermore, Groq's generous free tier offers up to 30 requests per minute, which is more than sufficient for the development and prototyping needs, making this a highly cost-effective choice.

    c. SentenceTransformers: A high-performance, open-source library for creating text embeddings. By running this locally, I can control the embedding process and avoid additional API costs or dependencies.

    d. Streamlit: Selected for its ability to rapidly develop and deploy interactive user interfaces, making it ideal for prototyping and delivering the user-facing application.

3. Core Logic and Orchestration:
The agent.py script orchestrates the entire process, directly fulfilling the project's core requirements in a repeatable pipeline for each user message:

    a. Intent Classification: The agent first determines the user's primary goal.
        * Intent LLM: A fast model (llama-3.1-8b-instant) used for classification.

    b. Context Gathering:
        * Memory & Ticket Retrieval: The agent checks for an active_ticket_id and uses database.py functions to fetch ticket details. This fulfills the requirement to "retrieve Ticket details from System of Record."

        * RAG Retrieval: The agent uses a (potentially refined) search query to perform a vector search against the pg_docs knowledge base. This fulfills the requirement to "identify relevant information from knowledge base."

    c. Response Synthesis: The agent sends the complete context (ticket data, articles, history) to the powerful Synthesis LLM. Governed by a robust system prompt, the LLM synthesizes a direct, helpful, and conversational response. This fulfills the requirement to "address customer concerns."
        * Synthesis LLM: A powerful model (llama-3.3-70b-versatile) for generating final responses.

    d. Memory Update: The turn is saved to the graph database, ensuring the agent is ready for the next interaction.

    e. Data Sources:
        * PostgreSQL Database (customer_support_kb):
            * tickets table (System of Record).
            * pg_docs table with pgvector for the knowledge base.
        * Apache AGE Graph: Stores the conversation history (Long-Term Memory).



